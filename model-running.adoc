= Running a Model in OpenShift.

== Overview

Now that we have a trained model, we can go ahead and deploy our application and start feeding it data. If we have trained our model properly, we should get fairly accurate predictions about the data we are feeding it.

In this section, we'll be building out a data pipeline that will analyze incoming Xray images, make an assessement of the risk of pneumonia, and optionally anonymize some images based on the certainty of this assessment.

This pipeline uses several tools and features. Here's a high level overview of the tasks we'll be completing:

 * Deploy a minimal Kafka Cluster and create a Kafka Topic with the AMQ Streams operator.

* Create object buckets in S3.

* Create a Serverless Service and a KafkaSource listener to process events from the Kafka topic.

* Create Grafana Data Sources and Dashboards to monitor our pipeline.


== Log into OpenShift

We'll start by logging our terminal session in Jupyterhub into OpenShift so that we can use the OpenShift cli, 'oc'. For more information on the OpenShift CLI, see https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html.

To log into OpenShift, run the following command in the terminal.

[source,sh,role="copypaste"]
----
oc login
----

You should see something like this (the project names may be different):

[source,none]
----
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * {{ USER_ID }}-notebooks

Using project "{{ USER_ID }}-notebooks".
Welcome! See 'oc help' to get started.
----

We'll come back to the OpenShift CLI later on when we get ready to deploy our applications.

== Deploy the Infrastructure for our model

Next, we'll deploy the infrastructure that we need for our lab.

=== Deploying Config Maps

The data pipeline we're deploying requires some shared configuration in our namespace.

To create the config maps, start by navigating to the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-notebooks[Topology View^]. Click on "Config Maps" and Select "Create Config Map" in the top right corner.

image::create-configmap.png[create-configmap, 700]

Delete the contents of the window and paste the following

[source,yaml,role="copypaste"]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: buckets-config
data:
  bucket-base-name: 'images'
  bucket-source: 'https://s3.us-east-1.amazonaws.com/com.redhat.csds.guimou.xray-source'
----

Click on Create.

Then click on "Create Config Map" again to create a second config map.

[source,yaml,role="copypaste"]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: service-point
data:
  url-external: 'ceph-nano-{{ USER_ID }}-notebooks.{{ ROUTE_SUBDOMAIN }}' # No trailing /
  url: 'http://ceph-nano.svc.cluster.local' # No trailing /
----

Finally, create one more config map to configure the database host:

[source,yaml,role="copypaste"]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-host
data:
  url: xraylabdb
----

== Deploying the Database

This database will hold some information on the images being uploaded, processed, and anonymized. This is basically their names, the model version with which they are analyzed, and a timestamp.
All these information will be used on our Grafana Dashboard that displays last ten images from each category.

We'll be using a MariaDB database for this. To deploy the database, click on +Add on the left menu from the console view. Click on Database. Select "MariaDB (Ephemeral)" and then click "Instantiate Template".

Replace the following values:

Database Service Name: xraylabdb
Username: xraylab
Password: xraylab
Root Password: xraylab
Database Name: xraylabdb

image::instantiate-mariadb.png[instantiate-mariadb, 700]

Wait for the database to roll out. You should see the circles in the topology turn dark blue.

image::topology-xraylabdb.png[topology-xraylabdb, 700]

=== Database configuration

We now have a database and a schema, but we must initialize it with some tables. To configure the database, follow these steps.

Connect to the database pod by running the following commands in the Jupyterhub terminal window:
[source,bash,subs="{markup-in-source}",role=execute]
----
oc rsh $(oc get pods | grep xraylabdb | grep Running | awk '{print $1}')
----

Your Terminal prompt is now the one from the database Pod. It should display:
[source,bash,subs="{markup-in-source}"]
----
sh-4.2$
----

.Connect to MariaDB
[source,bash,subs="{markup-in-source}",role=execute]
----
mysql -u root
----

Your Terminal prompt is now the one from the MySQL engine.

.Select the xraylabdb database
[source,sql,subs="{markup-in-source}",role=execute]
----
USE xraylabdb;
----

For the following commands, you can copy/paste all lines at once in the mysql prompt. 

.Initialize tables
[source,sql,subs="{markup-in-source}",role=execute]
----
DROP TABLE images_uploaded;
DROP TABLE images_processed;
DROP TABLE images_anonymized;

CREATE TABLE images_uploaded(time TIMESTAMP, name VARCHAR(255));
CREATE TABLE images_processed(time TIMESTAMP, name VARCHAR(255), model VARCHAR(10), label VARCHAR(20));
CREATE TABLE images_anonymized(time TIMESTAMP, name VARCHAR(255));

INSERT INTO images_uploaded(time,name) SELECT CURRENT_TIMESTAMP(), '';
INSERT INTO images_processed(time,name,model,label) SELECT CURRENT_TIMESTAMP(), '', '','';
INSERT INTO images_anonymized(time,name) SELECT CURRENT_TIMESTAMP(), '';
----

.Exit mysql prompt
[source,sql,subs="{markup-in-source}",role=execute]
----
exit;
----

Your Terminal prompt is now the one from the database Pod!

.Exit database pod
[source,bash,subs="{markup-in-source}",role=execute]
----
exit
----

=== Create the Kafka Cluster and Topic

Let's create a **Kafka cluster**. Click *+Add* on the left in the OpenShift topology view, and on the _From Catalog_ box on the project overview:

Type in `kafka` in the search box, and click on the *Kafka*:

image::kafka-catalog.png[kafka-catalog, 700]

Click on *Create* and you will enter YAML editor that defines a *Kafka* Cluster. Keep the all values as-is then click on *Create* on the bottom.

The zookeeper and kafka clusters will roll out in the Topology view.

image::topology-kafka.png[topology-kafka, 700]

Next, we will create Kafka _Topic_. Click _Add > From Catalog_ again, type in `kafka topic` in the search box, and click on the *Kafka Topic*:

image::kafka-topic-catalog.png[kafka, 700]

Click on *Create* and you will enter YAML editor that defines a *KafkaTopic* object. Change the name to `xray-images` as shown then click on *Create* on the bottom.

image::create-kafka-topic.png[create-kafka-topic, 700]

The Kafka topic will not display on the OpenShift topology.

=== Configure the S3 buckets.

We'll run a notebook to configure the S3 buckets that our application would use. This notebook (as well as our application) uses the "boto3" library for Python to configure the buckets. We'll need to install boto3 in our Jupyterhub instance by running the following commands in our terminal:

[source,sh,role="copypaste"]
----
pip install boto3
----

You should see output similar to the following:

[source,sh,role="copypaste"]
----
Collecting boto3
  Downloading boto3-1.17.61-py2.py3-none-any.whl (131 kB)
     |████████████████████████████████| 131 kB 6.2 MB/s
Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/app-root/lib/python3.6/site-packages (from boto3) (0.10.0)
Collecting botocore<1.21.0,>=1.20.61
  Downloading botocore-1.20.61-py2.py3-none-any.whl (7.5 MB)
     |████████████████████████████████| 7.5 MB 15.3 MB/s
Collecting s3transfer<0.5.0,>=0.4.0
  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)
     |████████████████████████████████| 79 kB 85.1 MB/s
Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.61->boto3) (2.8.1)
Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/app-root/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.61->boto3) (1.25.11)
Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.61->boto3) (1.15.0)
Installing collected packages: botocore, s3transfer, boto3
  Attempting uninstall: botocore
    Found existing installation: botocore 1.17.44
    Uninstalling botocore-1.17.44:
      Successfully uninstalled botocore-1.17.44
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
aiobotocore 1.1.2 requires botocore<1.17.45,>=1.17.44, but you have botocore 1.20.61 which is incompatible.
Successfully installed boto3-1.17.61 botocore-1.20.61 s3transfer-0.4.2
----

Now that we've installed the module, navigate back to the jupyterhub notebooks and click on the "create_notifications.ipynb" notebook to launch it.

Walk through the notebook to create the buckets:

image::bucket-list.png[bucket-list, 700]

== Deploy the Model

Next we'll deploy the three services which will do the following steps in the pipeline:

1) Grab an X-Ray image and drop it into the incoming bucket.
2) Analyze the incoming image and tag it.
3) Display a processed image.

=== Deploy the Image Generator

=== Deploy the Risk Assessment Service

=== Deploy the Image Server
