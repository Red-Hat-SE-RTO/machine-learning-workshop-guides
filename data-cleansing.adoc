= Data Cleansing in OpenShift.

In this workshop, we'll be learning how to perform basic Data Science and Machine Learning using OpenShift. The OpenShift platform greatly simplifies deploying and using the tools that are typically used in most Data Science workflows today. In particular, we'll be using the http://opendatahub.io[Open Data Hub^] project to deploy most of the infrastructure we'll be using in the lab today.

== Introduction to Open Data Hub

Open Data Hub (ODH) is an open source project based on https://kubeflow.org/[Kubeflow^] that provides open source AI tools for running large and distributed AI workloads on OpenShift Container Platform. Currently, the Open Data Hub project provides open source tools for data storage, distributed AI and Machine Learning (ML) workflows, Jupyter Notebook development environment and monitoring. The Open Data Hub project roadmap offers a view on new tools and integration the project developers are planning to add.

Open Data Hub includes several open source components, which can be individially enabled. They include:

* Apache Airflow
* Apache Kafka
* Apache Spark
* Apache Superset
* Argo
* Grafana
* JupyterHub
* Prometheus
* Seldon

Open Data Hub itself is deployed on OpenShift via an https://www.openshift.com/learn/topics/operators[Operator^]. Operators on OpenShift are responsible for managing https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/[Custom Resources^]. Custom resources allow for OpenShift users to create higher level objects in Kubernetes than are available via the basic Kubernetes API. For example, the AMQ Streams Operator defines a custom resource called "Kafka", which allows us to create a Kafka cluster. Operators can define more than one Custom Resource - the AMQ Streams Operator also allows for the creation of a "KafkaTopic" on our "Kafka" cluster.

== Data Cleansing

The first step in developing a model for machine learning is often cleansing the data on which we're going to operate. In this section of the workshop, we'll learn how to perform basic data cleansing using a set of data available on the Internet. We'll download the data in CSV format and use the Python https://pandas.pydata.org/[Pandas^] library to clean and visualize the data inside of a Jupyter notebook.

https://jupyter.org/[Jupyter notebooks^] are widely used throughout the data science community to organize, run, and share code. Jupyter supports many programming languages, but the most common languages used in data science are Python and R. This workshops examples will all be in Python, as will the code we use to train and run our model.

In this first exercise, we'll be deploying a Jupyter Hub instance for our user on OpenShift and loading a notebook that performs some basic data cleansing and visualization exercizes on COVID-19 data from the https://covidtracking.com/[COVID Tracking project^].

=== 1. Access Your Development Environment

You will be using Red Hat CodeReady Workspaces, an online IDE based on https://www.eclipse.org/che/[Eclipse Che^]. *Changes to files are auto-saved every few seconds*, so you don’t need to explicitly save changes.

To get started, {{ ECLIPSE_CHE_URL }}[access the CodeReady Workspaces instance^] and log in using the username and password you’ve been assigned (e.g. `{{ USER_ID }}/{{ CHE_USER_PASSWORD }}`):

image::che-login.png[cdw, 700]

Once you log in, you’ll be placed on your personal dashboard. Click on the name of
the pre-created workspace on the left, as shown below (the name will be different depending on your assigned number).

image::crw-landing.png[cdw, 700]

You can also click on the name of the workspace in the center, and then click on the green {{ USER_ID}}-namespace that says _Open_ on the top right hand side of the screen:

image::crw-landing-start.png[cdw, 700]

After a minute or two, you’ll be placed in the workspace:

image::che-workspace.png[cdw, 900]

This IDE is based on Eclipse Che (which is in turn based on MicroSoft VS Code editor).

You can see icons on the left for navigating between project explorer, search, version control (e.g. Git), debugging, and other plugins.  You’ll use these during the course of this workshop. Feel free to click on them and see what they do:

image::crw-icons.png[cdw, 400]

[NOTE]
====
If things get weird or your browser appears, you can simply reload the browser tab to refresh the view.
====

Many features of CodeReady Workspaces are accessed via *Commands*. You can see a few of the commands listed with links on the home page (e.g. _New File.._, _Git Clone.._, and others).

If you ever need to run commands that you don't see in a menu, you can press kbd:[F1] to open the command window, or the more traditional kbd:[Control+SHIFT+P] (or kbd:[Command+SHIFT+P] on Mac OS X).

Let's import our first project. Click on **Git Clone..** (or type kbd:[F1], enter 'git' and click on the auto-completed _Git Clone.._ )

image::che-workspace-gitclone.png[cdw, 900]

Step through the prompts, using the following value for **Repository URL**. If you use *FireFox*, it may end up pasting extra spaces at the end, so just press backspace after pasting:

[source,none,role="copypaste"]
----
https://github.com/msolberg/machine-learning-workshop-labs.git
----

image::crw-clone-repo.png[crw,900]

Click on *Select Repository Location* then click on *Open in New Window*. It will reload your web browser immediately:

image::crw-add-workspace.png[crw, 900]

The project is imported into your workspace and is visible in the project explorer:

image::crw-clone-explorer.png[crw,900]

==== IMPORTANT: Check out proper Git branch

To make sure you're using the right version of the project files, run this command in a CodeReady Terminal:

[source,sh,role="copypaste"]
----
cd $CHE_PROJECTS_ROOT/machine-learning-workshop-labs && git checkout main
----

[NOTE]
====
The Terminal window in CodeReady Workspaces. You can open a terminal window for any of the containers running in your Developer workspace. For the rest of these labs, anytime you need to run a command in a terminal, you can use the **>_ New Terminal** command on the right:

image::codeready-workspace-terminal.png[codeready-workspace-terminal, 700]
====

== Deploy Open Data Hub

We'll be using Open Data Hub to deploy JupyterHub, Ceph Nano, and Spark. The Open Data Hub operator is already installed for you, but you'll need to deploy an instance of it into your project.

To deploy ODH, first open a new brower with the {{ CONSOLE_URL }}[OpenShift web console^]:

image::openshift_login.png[openshift_login, 700]

Login using:

* Username: `{{ USER_ID }}`
* Password: `{{ OPENSHIFT_USER_PASSWORD }}`

You will see a list of projects to which you have access:

image::openshift_landing.png[openshift_landing, 700]

Open the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-notebooks[Topology View^].

To deploy an instance of ODH, click *From Catalog* on the Add page.

image::from-catalog.png[from-catalog, 700]

Type in `Data` in the search box, and click on the *Open Data Hub*:

image::data-hub-operator.png[data-hub-operator, 700]

Click on *Create*. This will bring you to a screen which allows you to customize the components that you'd like to install.

Click on *YAML View*. Replace the YAML in the edit box with the following definition:

[source,yaml,role="copypaste"]
----
   # ODH uses the KfDef manifest format to specify what components will be included in the deployment
   apiVersion: kfdef.apps.kubeflow.org/v1
   kind: KfDef
   metadata:
     # The name of your deployment
     name: opendatahub
   # only the components listed in the `KFDef` resource will be deployed:
   spec:
     applications:
       # REQUIRED: This contains all of the common options used by all ODH components
       - kustomizeConfig:
           repoRef:
             name: manifests
             path: odh-common
         name: odh-common
       # Create the SecurityContextConstraint to grant the ceph-nano service account anyuid permissions
       - kustomizeConfig:
           repoRef:
             name: manifests
             path: ceph/object-storage/scc
         name: ceph-nano-scc
       # Deploy ceph-nano for minimal object storage running in a pod
       - kustomizeConfig:
           repoRef:
             name: manifests
             path: ceph/object-storage/nano
         name: ceph-nano
       # Deploy Radanalytics Spark Operator
       - kustomizeConfig:
           repoRef:
             name: manifests
             path: radanalyticsio/spark/cluster
         name: radanalyticsio-spark-cluster
       # Deploy Open Data Hub JupyterHub
       - kustomizeConfig:
           parameters:
             - name: s3_endpoint_url
               value: "http://ceph-nano-0"
           repoRef:
             name: manifests
             path: jupyterhub/jupyterhub
         name: jupyterhub
       # Deploy addtional Open Data Hub Jupyter notebooks
       - kustomizeConfig:
           overlays:
             - additional
           repoRef:
             name: manifests
             path: jupyterhub/notebook-images
         name: notebook-images
     # Reference to all of the git repo archives that contain component kustomize manifests
     repos:
       # Official Open Data Hub v1.0.0 component manifests repo
       # This shows that we will be deploying components from an archive of the odh-manifests repo tagged for v1.0.0
       - name: manifests
         uri: 'https://github.com/opendatahub-io/odh-manifests/tarball/v1.0.2'
----

image::create-kfdef.png[create-kfdef, 700]

Click *Create*. On the topology view, you should see the Open Data Hub Components being deployed.

image::deployed-odh.png[deployed-odh, 700]

Although your CodeReady workspace is running on the Kubernetes cluster, it’s running with a default restricted _Service Account_ that prevents you from creating most resource types. If you’ve completed other modules, you’re probably already logged in, but let’s login again: click on *Login to OpenShift*, and enter your given credentials:

* Username: `{{ USER_ID }}`
* Password: `{{ OPENSHIFT_USER_PASSWORD }}`

image::cmd-login.png[login,700]

You should see something like this (the project names may be different):

[source,none]
----
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * {{ USER_ID }}-notebooks

Using project "{{ USER_ID }}-notebooks".
Welcome! See 'oc help' to get started.
----

[NOTE]
====
After you log in using *Login to OpenShift*, the terminal is no longer usable as a regular terminal. You can close the terminal window. You will still be logged in when you open more terminals later!
====


== Buckets and Bucket Notifications creation

=== Connect to JupyterHub

Click on the jupyterhub icon and look for the "Routes" section. Click on the route listed there.

Just click on this link, a new tab will open. Click on the button *Sign in with OpenShift*, and use your OpenTLC credentials to connect.+
On the first connection, OpenShift will ask to authorize the application to access your username just click *Allow selected permissions*.

=== Launch Jupyter

On the *Spawner Options* page select the *s2i-minimal-notebook:3.6* image from the first dropdown (this should be the default image), and click *Spawn* at the bottom.

Your Jupyter environment will take 15-20 seconds to launch.

It will display a _File Explore like_ interface. Click on the *xraylab_notebooks.git* folder, then on the *georgia_covidtracking.ipynb* file, which will launch the notebook.

